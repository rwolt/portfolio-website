---
layout: "../../layouts/project.astro"
slug: "yt-semantic-search"
title: "Youtube Semantic Search"
stack: ['React', 'Typescript', 'Convex', 'tailwindcss']
summary: "Youtube Transcript Semantic Search allows users to create custom knowledge bases from YouTube videos. Users can then perform queries against that data using a traditional search interface or a natural language chat interface with citations, and view search results in a video player."
---
**Project Purpose and Goal:** In the Spring of 2023, I went to a local web development meetup where the talk was about using the OpenAI API to perform semantic search on a knowledge base by comparing embeddings. It was a simple demonstration using a set of question and answer documents as the data set, much like the FAQ on your businesses’ website. Like most anyone who was keeping up with the world of technology, I had used chatGPT and Google Bard, and was impressed with what generative language models could do, but something about this simple demo was even more mind blowing to me. In that moment I saw the possibilities of new kinds of apps that could be built. It got me thinking: what if a language model had access to a large corpus of data outside its training set, such as your company’s knowledge base or even your personal health data? To experiment with this, I decided to create a chat bot that experiment with the idea of querying documents in my own database and handing those documents to the chat completion API as context. I chose to use YouTube transcripts as a knowledge base, because the platform contains an enormous amount of information that is easily accessed via an API, and I wanted to create an element of interactivity that wouldn’t be possible if only using text documents as a knowledge base. 

**Web Stack and Explanation:**  Because the main purpose of this project was to learn to use the OpenAI API, and compare embeddings, I chose to use Convex as a back end. Convex is a back end as a service that allows you to write server functions which you then call from your front end using custom hooks. The connection between client and server use web sockets, which makes the UI reactive without have having to manually fetch data and manage it in local state. Convex also has functionality for vector search, which allowed me to store embeddings generated using the OpenAI API directly on the documents in the NoSQL database (it’s not actually a NoSQL database, it’s a “relational document” database). 

**Problems and Thought Process:** I encountered two major problems when building out this project. The first had to do with prompting the OpenAI chat completion API to respond consistently with valid JSON. In order to provide links to relevant portions of the video in the chat response, my system prompt to the chat completion API needed the response to always return valid JSON with a relevant source citation for each statement. This problem was mostly mitigated by modifying the system prompt to explicitly provide the JSON format to the chat model. One problem I had was related to the structure of the youtube transcript returned from the API. The transcript was divided into chunks that were about 6 seconds long. I need to combine each chunk into a larger chunk that was the right size to use as a document for the chat completion API. I looked into using some natural language processing tools to split the text into semantically related sections, but it was too much complexity for my first app dealing with text embeddings. I could also have used the metadata from the youtube video to split the text by chapter, if the video had chapters. This would have been easier, but not every video support chapters. In the end, I decided to simply split the text at 3000 characters, which is about 600 tokens when used inside the chat completion API. In order to deal with the chunks not necessarily being semantically related, I made a 10 percent overlap in the text. Since I would be using the top 4 documents as context, I hoped that any relevant text that was split across two documents would return both results, as they overlapped by 10 percent. In the end, this was a good compromise, although the time codes do not point to the exact start of each semantically related section (the same way a chapter marker does). If I had to repeat this in the future, I would use NLP tools or split the text at chapter markers to get more accurate time codes for the clips. 

**Lessons Learned:**  This project taught me the importance of data structures and algorithms. I had a few cases while building this application where I needed to do some more complex data manipulation. For example, the transcript that I received from youtube was in 6 second chunks. I needed to combine these smaller chunks into chunks of text long enough to provide context to the chat completion API. Another situation like this was creating a list of user collections or a list of videos within a collection. The solution I had to map over all documents and keep track of unique video names worked, but once I started to get a large amount of document in my collection, it would crash the app. This project also taught me how to use Typescript with React. I got used to some of the React types, and also some useful patterns such as using type guards to check if a variable is undefined.